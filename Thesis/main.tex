% Do not change the options here
\documentclass[bsc,frontabs,parskip,deptreport]{infthesis}

\usepackage{graphicx}
\usepackage{subfig}
\usepackage[%  
    colorlinks=true,
    pdfborder={0 0 0},
    linkcolor=black,
    citecolor=blue
]{hyperref}
\usepackage[authoryear,comma]{natbib}
\usepackage{csvsimple}
\usepackage{fancyvrb}
% \setcitestyle{authoryear,comma,open={(},close={)}} 

\def\sectionautorefname{Section}	
\def\chapterautorefname{Chapter}
\def\subsectionautorefname{Section}
\def\subfigureautorefname{Figure}
\def\tableautorefname{Table}
\newcommand{\coderepo}{\href{https://github.com/LasseWolter/laughter-detection-icsi}{codebase} }

\begin{document}
\begin{preliminary}
\title{A Zoom filter for applause and laughter}

\author{Lasse Wolter}

% to choose your course
% please un-comment just one of the following
%\course{Artificial Intelligence}
\course{Artificial Intelligence and Computer Science}
%\course{Artificial Intelligence and Mathematics}
%\course{Artificial Intelligence and Software Engineering}
%\course{Artificial Intelligence with Management}
%\course{Cognitive Science}
%\course{Computer Science}
%\course{Computer Science and Management Science}
%\course{Computer Science and Mathematics}
%\course{Computer Science and Physics}
%\course{Computer Science with Management}
%\course{Software Engineering}
%\course{Software Engineering with Management}

\project{4th Year Project Report}

\date{\today}

\abstract{

% This skeleton demonstrates how to use the \texttt{infthesis} style for
% undergraduate dissertations in the School of Informatics. It also emphasises the
% page limit, and that you must not deviate from the required style.
% The file \texttt{skeleton.tex} generates this document and can be used as a
% starting point for your thesis. The abstract should summarise your report and
% fit in the space on the first page.
}

\maketitle

\section*{Acknowledgements}


\tableofcontents
\end{preliminary}


% The preliminary material of your report should contain:
% \begin{itemize}
% \item
% The title page.
% \item
% An abstract page.
% \item
% Optionally an acknowledgements page.
% \item
% The table of contents.
% \end{itemize}

\chapter{Introduction}


The main achievements of this project are:
\begin{enumerate}
  \item Evaluation of existing laughter detection model on the whole ICSI meeting corpus (\autoref{cha:model-evaluation})
  \item Creation of a machine learning pipeline for laughter recognition using the ICSI meeting corpus, consisting of two parts:
  \begin{enumerate}
      \item a data pipeline: transforming the raw data into features for model training (\autoref{sec:ml-data-pipeline})
      \item an evaluation pipeline: evaluating model outputs and visualisation of results
  \end{enumerate}
  \item Investigation of impact of training data on model performance (\autoref{cha:experiments})
\end{enumerate}

Throughout the report I regularly mention discussions. These refer to the biweekly meetings with my first and second supervisor. The meetings consisted of a short presentation by myself and a discussion about intermediate results and next steps. Details about each of these meetings can be found as part of \href{https://github.com/LasseWolter/honours-project-21-22/tree/main/Meeting_Notes}{this github repository} which contains slides, research done, general notes and some initial code snippets I worked on. 
The final laughter recognition codebase is published \href{https://github.com/LasseWolter/laughter-detection-icsi}{here}.

\chapter{Background} \label{cha:bg}
Automatic applause and laughter detection isn't a new idea. Nevertheless, the research in these two areas is usually done separately. There are a few papers that cover both, one of the most popular ones being from Cai et al. \citep{cai2003highlight} who used sound effect detection - which included laughter and applause - for video summarisation and highlight extraction.
Apart from this and a few other papers most research only covers one of the two domains, either applause or laughter. Thus, the review covers them in separate sections. A third section for evaluation metrics was added to introduce some of the core metrics used in this field. 

\section{Laughter Detection} \label{sec:bg-laughter}
The investigation of laughter's acoustic features reaches back three decades \citep{bickley1992acoustic}.
In the early 2000s, the first attempts of laughter detection in conversational speech classified presegmented audio data \citep{kennedy2004laughter, truong2005automatic}. These models could only state whether laughter occurred within a given segment. The determination of segment boundaries of laughter events was not considered. 
Corpora used in these papers include the development data from the 2004 Spring NIST Rich Transcription Evaluation \citep{ldcnistcorpus}, the Dutch CGN corpus \citep{oostdijk2000spoken} as well as the ICSI Meeting Recorder corpus \citep{morgan2001meeting}. 

Two examples of such classification of presegmented audio data are \citeauthor{kennedy2004laughter} and \citeauthor{truong2005automatic}. 
Even though both of these models classified presegmented audio there were some significant differences. 
Firstly, the definition of laughter and the motivation behind the research differed.
While Kennedy and Ellis define laughter events as 'points in the meeting where more than one person laughs', Truong and Van Leeuwen considered one person laughing as a laughter event.
Kennedy and Ellis did not specify a particular motivation whereas Truong and Van Leeuwen's long-term goal was the investigation of 'paralinguistic events' - laughter being one of them - to classify the speaker's emotional state.   

Secondly, there was a difference between the best performing features and predictors.  
\citeauthor{kennedy2004laughter} got the best results using Mel Frequency Cepstral Coefficients (MFCCs) as features and a Support Vector Machine (SVM) for decision making. 
In contrast, \citeauthor{truong2005automatic} used Perceptual Linear Prediction (PLP) features with Gaussian Mixture Models (GMM) for classification. 

Truong and Leeuwen continued their research \citep{truong2007automatic} and stated that spectral features alone (such as PLP and MFCCs) can be used to discriminate between laughter and speech but a significant improvement can be achieved when using prosodic features - features relating to rhythm and intonation.

In 2006, \citeauthor{knox2006automatic} were the first to do laughter recognition without the need for presegmented audio data.  
They worked with the Bmr-subset of the ICSI Meeting corpus \citep{morgan2001meeting} to be able to compare their results to previous research done on this dataset \citep{kennedy2004laughter, truong2005automatic, truong2007automatic}.

Their goal was to obtain accurate interval boundaries of laughter segments.
One limiting factor for the precision of these boundaries is the frame size. 
When Knox and Mirghafori first experimented with SVMs similar to Kennedy and Ellis \citep{kennedy2004laughter}, they realised that the time to compute the features and train the SVM increased significantly with decreasing frame size. 
This is because aggregate statistics need to be calculated and stored for each frame of training data.  
On the contrary, a neural network trained with features from a context window can directly use the features of each frame without aggregating them.  

To obtain frame-level training labels for the ICSI dataset Knox and Mirghafori first removed all segments that contained both speech and laughter under a single start and end time because the laughter specific interval boundaries could not be identified in those segments. 
The remaining audio data could be accurately separated into laughter and non-laughter segments which were then divided into 10ms frames and labeled accordingly.

The model was trained to classify each 10ms frame using a window of 75 frames around the target frame - 37 before and 37 after the frame (\autoref{fig:knox_window}).
This way an accurate prediction of laughter boundaries of up to 10ms was possible. 
The features investigated by Knox and Mirghafori are MFCCs, AC PEAK, F0 and RMS as well as their corresponding delta and delta-delta features\footnote{first and second derivatives of the features}.
They first trained four separate neural networks each one taking one of the mentioned features as input.
Afterwards, they combined the different models by using their output as input for another, smaller neural network.
The best results were obtained by combining the outputs of three NN-systems which used delta MFCCs, AC PEAK and F0 as features, respectively.
By training and evaluating separate neural networks for each type of feature first, Knox and Mirghafori observed that MFCCs have the most discriminative power which aligns with prior research by Kennedy and Ellis \citep{kennedy2004laughter}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=10cm]{imgs/Knox_window.png}
    \caption{Frame window used by Knox and Mirghafori}
    \label{fig:knox_window}
\end{figure}

A survey from 2016 \citep{cosentino2016quantitative} investigates research on laughter and laughter detection in different fields up to that time.
This survey covers different detection methods using visual, acoustic and sensory data.
\citeauthor{cosentino2016quantitative} found that using solely acoustic features best performance is obtained by using MFCCs and PLPs as features and combining the resulting classifier with ones that also use prosodic features.
This aligns with findings from prior research \citep{truong2007automatic, knox2006automatic}.

More recent research by Gillick et al. \citep{gillick2021robust} finds that features learned from spectrograms using deep convolutional networks outperform previous approaches based on MFCCs.
They hypothesised that models using MFCCs are more prone to pick up surface level characteristics of sound and thus, will be more sensitive to variations like background noise.
Gillick et al. expected that features learned using a CNN are more representative of the actual laughter and thus, more robust to different environments. 
To test this hypothesis they trained three different models: a baseline feed-forward neural network with MFCC features from their existing research \citep{ryokai2018capturing}, a ResNet model working directly on spectrogram data and the same ResNet model using augmented features. Augmentations applied include pitch-shifting, time-stretching, reverberation and SpecAugment \citep{park2019specaugment}. SpecAugment is a simple method to augment spectrograms by time warping, frequency masking and time masking. \autoref{fig:spec-augment} shows that frequency and time masking is done by blocking out certain spectrogram regions, vertically or horizontally. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=13cm]{imgs/examples/spec_augment_example.png}
    \caption{SpecAugment example}
    \label{fig:spec-augment}
\end{figure}

\citeauthor{gillick2021robust} used two corpora: Switchboard \citep{switchboard-corpus} and AudioSet \citep{googleaudioset}.
In contrast to prior work, they also evaluated the models on the corpus that it was not trained on.
This mismatch between training and testing data resembles the difference between clean audio and real-world test data.
The results (\autoref{fig:gillick-results}) show that the baseline is outperformed by the ResNet architecture in every metric for every evaluation, except the recall for the model trained on AudioSet and evaluated on the Switchboard test data. 
Gillick et al. conclude that their ResNet-based model provides a robust, state-of-the-art laughter-detection algorithm that mitigates some of the problems of noisy real-world environments. This is one of the reasons why it is used as a basis for this project.

\begin{figure}[h!]
    \centering
    \includegraphics[width=14cm]{imgs/results/gillick_et_al.png}
    \caption{Results by Gillick et al. with 95\% confidence intervals. SLD=Strongly labeled data, WLD=weakly labeled data}
    \label{fig:gillick-results}
\end{figure}

\section{Applause}
The following review is less exhaustive because we decided to focus on laughter detection for now. Applause detection is a possible extension of the project. 
More information on this decision can be found (\autoref{sec:model-and-data}).

As mentioned at the beginning of this chapter, in 2003 Cai et al. \citep{cai2003highlight} worked on the detection of sound events, including applause.
They used perceptual features and MFFCs as features, Hidden Markov Models (HMM) and Gaussian Mixture models (GMM) to model sound and log likelihood for decision making.
Cai et al. evaluated their system on a testing set consisting of 2 hours of video material from different programs.
For applause specifically, they achieved a precision of 87.37\% and a recall of 92.00\%.

In contrast, Uhle \citep{uhle2011applause} used MFCCs and low-level descriptors (LLD) to represent sound and passed these features to an MLP or an SVM for classification.
The difference between the MLP and SVM classifier was rather small.
Uhle worked with a relatively small dataset consisting of 210 segments of 9-30s length. This equates to a total length between 31.5 min and 105 min.
On a rather small test (10\% of this dataset) Uhle achieved an accuracy of 95\% with a precision of 96.51\% and recall of 97.65\%.

A less complex approach for applause detection was presented by Li et al. \citep{li2009characteristics}.
They used a manually created 4-layer decision tree to classify a given sound input as applause or not-applause.
Testing their model on 50 hours of meeting speech containing 500 applause segments ranging from 0.8 to 36s they were able to retrieve 491 of the 500 applause segments while incorrectly retrieving 38 non-applause segments.
This equates to a recall of 98.2\% and precision of 92.82\%. Li et al. also compared their less complex model to the HMM model proposed by Cai et al.\citep{cai2003highlight} and outperformed it while using less computational time.

Manoj et al. \citep{manoj2011novel} proposed another approach based on manually created decision trees. They also compared it to a more complex model similar to the one proposed by Cai et al. \citep{cai2003highlight} - with the difference that this model only used GMMs, no HMMs.
Even though the decision tree stages were different to Li et al. \citep{li2009characteristics} the findings are similar. The decision tree outperforms the more complex method using MFCCs and GMMs.  

\section{Evaluation metrics} \label{theory}
\subsection{Accuracy, Precision and Recall} \label{sec:acc-prec-rec}
Accuracy, precision and recall are standard metrics for performance evaluation.
All three metrics are calculated by comparing the predicted class labels to the true class labels.
This section states the formulae, their meaning for our use case and the advantages of one metric over the other.
Let:
\begin{enumerate}
    \item $TP$: True Positives - laughter samples correctly classified as laughter
    \item $FP$: False Positives - non-laughter samples incorrectly classified as laughter
    \item $TN$: True Negatives - non-laughter samples correctly classified as non-laughter
    \item $FN$: False Negatives - laughter samples incorrectly classified as non-laughter
\end{enumerate}
\textbf{Accuracy}: Percentage of correctly classified samples (laughter and non-laughter) over the total number of samples.
$$Accuracy = \frac{TP+TN}{TP+TN+FP+FN}$$
\textbf{Precision}: Percentage of correctly classified laughter samples over all samples predicted as laughter.
$$Precision = \frac{TP}{TP+FP}$$
\textbf{Recall}: Percentage of correctly classified laughter samples over all laughter-samples.
$$Recall = \frac{TP}{TP+FN}$$

For our use case, we are interested in continuous audio. All metrics presented above need some fixed unit for their calculation. We make use of segment-based-metrics suggested by \citep{mesaros2016metrics} which splits a continuous audio stream into fixed-sized segments, e.g. 10ms. \citeauthor{gillick2021robust} use the same approach for their evaluations.

Accuracy is a good metric if classes are balanced. In our use case, the two classes are very imbalanced. Laughter only occurs a few times in a long audio stream. If the model predicted all segments as non-laughter a large number of true negatives would skew the accuracy. A model with no skill can get high accuracy.
For such cases also considering precision and recall is helpful. Considering the same example, the recall will be 0\% because the model did not retrieve any laughter segments correctly. 

To conclude, for our case precision and recall are more expressive metrics than accuracy.

\subsection{Binary Cross Entropy Loss}
In comparison to accuracy, precision and recall (\autoref{sec:acc-prec-rec}) the loss of a model is computed from the raw probabilities output by the model, not the predicted labels.

The machine learning model outputs a probability between 0 and 1. Only applying a threshold turns this probability into a class label. This threshold is a variable parameter.
For example, with a threshold of 0.5, all predicted probabilities of 0.5 and higher are output as class 1, in our case laughter. All probabilities below 0.5 are output as non-laughter. 
Varying this threshold changes the predicted labels and thus, yields a different performance in terms of precision and recall. 

In contrast, the loss of a model's output is independent of the threshold as it is calculated directly from the probabilities.
When a machine learning algorithm learns how to improve its predictions it minimises this loss. There are different functions for calculating loss.
A typical approach for classification problems is the Cross Entropy Loss. Since our use case only has two classes (laughter and non-laughter) we can use Binary Cross Entropy Loss. 

Assume we have two vectors $P$ and $Y$ of length $m$. For each of the $m$ samples, $P$ contains the probability of the sample being laughter whereas $Y$ contains its true class (laughter $\Rightarrow y=1$, non-laughter $ \Rightarrow y=0$).
Given this, the model's Binary Cross Entropy Loss is given by:
$$ L(P,Y) = \sum_{n=1}^{m}  -{(y*\log(p) + (1 - y)*\log(1 - p))}$$

To understand the meaning of this equation we look at a single sample with predicted probability \texttt{p} and a true class value of \texttt{y}. Its Binary Entropy Loss is given by: 
$$ L(p,y) = -{(y*\log(p) + (1 - y)*\log(1 - p))} $$
We know that one of the terms $y$ and $(1-y)$ will always be zero.
Thus, we are left with the negative log of the probability that the sample belongs to the true class, e.g. a sample with $p=0.8$ is predicted to be laughter with an 80\% chance and non-laughter with a 20\% chance. Negative log ensures that $L(x,y) > 1  \forall x $. 
Due to the logarithmic nature of this loss-function confidently misclassified examples (e.g. $p=0.1$) are punished more heavily than unconfidently misclassified examples (e.g. $p=0.4$).

To summarise, the Binary Cross Entropy Loss captures how far off the predictions of the model are from the true values. It sums negative log probabilities of belonging to the true class and punishes confidently misclassified samples more heavily.

\subsection{Confusion Matrix} \label{sec:conf-matrix}
A confusion matrix is a good way to identify misclassifications. In a multi-class problem, it shows the true label on one axis and the predicted label on the other. It can either show the actual counts of predictions or normalised values between 0 and 1. \autoref{fig:example-conf-matrix} shows a normalised confusion matrix for a classification problem with three classes.
For laughter recognition we only have two classes. Due to subclasses of non-laughter (\autoref{table:segment-types}) we can still calculate the confusion for laughter-samples. For non-laughter classifications it's unknown if the model considered a segment as speech, noise or silence but for laughter classifications this can be evaluated. In our case we plot several rows in one plot to compare the confusion for different thresholds, e.g. \autoref{fig:conf-matrix-rand}
\begin{figure}
    \centering
    \includegraphics[width=6cm]{imgs/examples/exmaple_confusion_matrix.png}
    \caption{Normalised confusion matrix (source: \citeauthor{confmatrixscikit})}
    \label{fig:example-conf-matrix}
\end{figure}

\chapter{Model evaluation} \label{cha:model-evaluation}

\section{Model and data selection}\label{sec:model-and-data}
After having done the initial research we decided to focus solely on laughter detection. 
There are two main reasons for this: the type of previous research and open source code available.  
As mentioned in the \autoref{cha:bg} most research in the field of laughter and applause detection is done separately. 
Thus, implementing a detection algorithm from existing research requires merging different models. 
For a combined model to work, a good understanding of both domains is essential.
We decided that this is most easily attained by focusing on one domain first. 
We chose to focus on laughter instead of applause because there was an open-source repository available that showcased an existing state-of-the-art laughter detection algorithm \citep{gillick-codebase}.

The domain in video conferences is separate single person audio tracks usually recorded in a relatively quiet environment.
To investigate how the existing model might perform in this domain, we decided to evaluate it on the ICSI meeting corpus \citep{morgan2001meeting}. 
We initially considered the use of AudioSet \citep{googleaudioset} as a possible evaluation corpus. After some discussion, we discarded this idea because of a mismatch with the domain for our project. AudioSet  consists of 10s audio segments from Youtube videos which were labeled according to a fixed set of classes - including laughter. Nevertheless, the domain of Youtube videos varies drastically. It can be a recording of someone laughing in a silent environment inside a room or someone laughing in a very noisy environment outside. This variety of environments might help for generalising laughter detection but is not suitable for our project.
In contrast, the ICSI corpus is a good match to our domain as it solely consists of meeting speech recorded with both close-distance and table-top microphones. 
We only make use of the close-distance microphone recordings which capture the audio of a single meeting participant.
The ICSI corpus provides transcriptions for all its participants containing speech as well as \texttt{Vocal} and \texttt{NonVocal} tags for sounds like laughter and door squeaks, respectively.
There are two further reasons for using the ICSI corpus: its size and its licence.
The ICSI corpus has a total length of 72h and around 3.9h of total laughter duration suitably transcribed for our use case (\autoref{sec:preprocessing}).
The licence is \href{https://creativecommons.org/licenses/by/4.0/legalcode}{CC BY 4.0} which means that it can be used for commercial projects like a video meeting extension.


\section{Preprocessing} \label{sec:preprocessing}
Before the model by Gillick et al. can be evaluated on the ICSI corpus, some preprocessing of the corpus data was needed.
At first, the 72 meetings were split into training, development and test set to minimise speaker overlap. The partitioning follows the structure from \citeauthor{renals2014neural}. It uses meetings \texttt{(Bmr021, Bns001)} as development set and meetings \texttt{(Bmr013, Bmr018, Bmr021)} as test set.
After this, all laughter segments needed to be filtered out from the raw data to accurately determine correct classifications. 
Lastly, I divided non-laughter segments into subclasses to get a more rigorous analysis of misclassifications.

\subsection{Filter out laughter only segments} \label{subsec:filter-laughter}
ICSI transcriptions are split into segments denoted by a start and end time.
Consequently laughter segments co-occurring to speech in one segment cannot be precisely identified.
Such laughter transcriptions are also called weakly labeled. Weakly labeled means that only the presence or absence of an event is given, whereas strongly labeled data states precise boundaries.
Strongly labeled data is key for this project because our model should identify laughter segments in a continuous audio stream.
Filtering out strongly labeled laughter segments resembles Knox and Mirghafori's approach to obtain frame-level training data \citep{knox2006automatic} (\autoref{sec:bg-laughter}). 
All weakly labeled segments containing laughter are categorised as invalid and disregarded from the evaluation, a total of 3765 segments.
The remaining 8415 laughter segments have a total duration of 3.9h. \autoref{table:segment-types} shows how this compares to the fraction of other types of audio in the ICSI corpus.

Alternatively, I could have created an ASR model to align weakly-labeled laughter events with their audio counterpart. 
I did not follow this approach for two reasons: it would have been more time consuming and the amount of data achieved with the prior approach was sufficient. 
In general, it is always better to have more training data though. Thus, future projects might consider this approach to gain strongly labeled data for the whole corpus and make use of the additional laughter occurrences contained in the invalid segments. 

\subsection{Subclasses of non-laughter}
In addition to the obvious class division between laughter and non-laughter, we can divide segments into smaller subclasses. This becomes useful when evaluating classification (see \autoref{cha:experiments}).
One subclass encompasses the invalid segments mentioned in \autoref{subsec:filter-laughter}. 
Further, we can distinguish between speech, noise and silence.
To summarise there are five subclasses:
\begin{enumerate}
    \item \textbf{laughter}: segments only containing laughter
    \item \textbf{non-laughter}
    \begin{enumerate}
        \item \textit{speech}: segments only containing speech
        \item \textit{invalid}: segments where laughter occurs next to other sounds (see \autoref{sec:preprocessing})
        \item \textit{noise}: segments containing vocal-sounds like coughing, non-vocal sounds like chair squeaks or a mixture of sounds and speech in one segment
        \item \textit{silence}: segments for which no transcription exists
    \end{enumerate}
\end{enumerate}
Note that these subclasses of non-laughter are not classified by the model. The model is a binary classifier. The distinction within the non-laughter class is merely used for evaluation.
Thus, the confusion-matrices in \autoref{sec:exp1-res} are only for laughter misclassifications as described in \autoref{sec:conf-matrix}.
A breakdown of the amount of each subclass is shown in \autoref{table:segment-types}.

\begin{table}[h!]
\begin{tabular}{c|c|l}
     subclass & number of segments & total duration \\
     \hline
     laughter & 8415 & 3.9 h \\ 
     invalid & 3765 & 3.7 h \\
     speech & 98418 & 64.1 h \\
     noise & 18224 & 10.9 h \\
     silence & 97431 & 681.9 h 
\end{tabular}
\centering
\caption{Occurrences and total duration of each subclass}
\label{table:segment-types}
\end{table}

\section{First evaluation}
In contrast to some existing research \citep{kennedy2004laughter, knox2006automatic} which only used a subset of the ICSI corpus, I evaluated the whole ICSI dataset.
I ran the pre-trained detection model by Gillick et al. with four different thresholds for each channel of the 75 meetings - a total of 468 audio tracks. 
Results of three different evaluation algorithms are displayed in \autoref{tab:initial-eval}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    & \multicolumn{2}{|c|}{initial eval} & \multicolumn{2}{|c|}{considering invalid segs} & \multicolumn{2}{|c|}{final eval}  \\
    \hline 
    threshold & precision & recall & precision & recall & precision & recall  \\
    \hline
        0.2 &  14.88\% & 31.53\% & 20.02\% & 37.21\%  & 20.86\% & 45.58\%\\
        0.4 &  33.26\% & 17.85\% & 48.39\% & 20.74\%  & 50.26\% & 27.77\%\\
        0.6 &  49.81\% & 8.05\% & 73.68\% & 9.14\%    & 73.77\% & 13.35\% \\
        0.8 &  63.08\% & 3.01\% & 91.44\% & 3.33\%    & 84.53\% & 4.76\% \\
     \hline
    \end{tabular}
    \caption{Initial and corrected evaluation on 468 audio tracks}
    \label{tab:initial-eval}
\end{table}


In the initial evaluation, the maximum recall achieved was 31.53\%. We expected that a low threshold like 0.2 would yield low precision. Getting such a low recall, however, was surprising. 
After some discussion, I realised that the invalid laughter segments were not excluded from the predictions. This meant that the model could classify a laughter occurrence correctly but the evaluation method would consider it as false because this laughter occurrence was discarded during preprocessing. This discrepancy in preprocessing was easily fixed and improved the precision significantly (\autoref{tab:initial-eval}). Nevertheless, the recall was not affected as much. 
Lastly, the third column of \autoref{tab:initial-eval} shows the final metrics. 
These final metrics include two significant changes: dealing with probabilities out of the range $(0,1)$ and using the weighted average. 

The evaluation code used by Gillick et al. sometimes yielded probabilities smaller than 0 and larger than 1. 
This bug was caused by a low-pass filter implemented in \verb|laugh_segmenter.py|. I did not investigate this further because neither the code nor the paper stated why this filter was necessary. Thus, I removed it. which fixed the issue.

I also realised that the use of a weighted average has a significant impact on the evaluation. The first two evaluations in \autoref{tab:initial-eval} are calculated by averaging the precision and recall of each meeting.
These results can be misleading because some meetings contain significantly more laughter occurrences than others, also due to the different lengths of the meetings. \autoref{fig:transc-laughter-distribution} shows that the total duration of laughter occurrences ranges from $<20s$ to $>950s=15min$.
For our use case, it makes sense to take the weighted average because we are interested in the total duration of correctly and incorrectly predicted laughter events, the artificial unit of meetings is an unimportant distinction to us.

Thus, the third evaluation shown in \autoref{tab:initial-eval} should be considered correct and is the one referred to in the rest of this thesis.

\begin{figure}
    \centering
    \includegraphics[width=13cm]{imgs/distributions/transcribed_laughter_time_distribution.png}
    \caption{Distribution of transcribed laughter duration per meeting [in s]; bin-width=20s}
    \label{fig:transc-laughter-distribution}
\end{figure}

To get a better understanding of the performance \autoref{tab:practical-example} shows what the evaluation metrics would mean in a real system. We assume a meeting that lasts 1 hour and contains a total duration of 4 minutes of laughter. 
Using any of the investigated thresholds would yield an unusable system.
We will briefly examine the two extreme cases. If the threshold was really low, e.g. 0.2, 1:49 min of laughter would be correctly retrieved. Despite this being not even half of the laughter events that occurred we also retrieve almost 7 minutes of noise.  
On the other hand, choosing a high threshold, e.g. 0.8, yields very high precision and thus, only two seconds of noise are transmitted. However, the system merely retrieves 14 seconds of the four minutes of laughter. With such a low percentage of retrieved laughter events there participants can just mute themselves entirely. 

\begin{table}[]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
      \hline
      threshold & precision & recall & predicted & actual laughter & noise \\
      \hline
      0.2 &  20.86\% & 45.58\% & 8:44 min & 1:49 min & 6:56 min \\
      0.4 &  50.26\% & 27.77\% & 7:16 min & 2:01 min & 5:15 min \\
      0.6 &  73.77\% & 13.35\% & 0:43 min & 0:32 min & 0:11 min \\
      0.8 &  84.53\% & 4.76\% & 0:14 min & 0:12 min & 0:02 min \\
      \hline
    \end{tabular}
    \caption{Practical example using weighted average values from \autoref{tab:initial-eval}}
    \label{tab:practical-example}
\end{table}


A possible reason for such a bad performance on the ICSI corpus is a data mismatch between training and evaluation data. The pre-trained model by Gillick et al. \citep{gillick2021robust} was trained on the Switchboard corpus \citep{switchboard-corpus}. The Switchboard corpus records telephone conversations between two participants at a sample rate of 8000hz. The ICSI corpus records meeting speech of multiple participants at a sample rate of 16000hz. For prediction, all audio tracks are down-sampled to 8000hz which results in losing lots of the original data. 
Further, the Switchboard dataset has one audio track containing the audio from both participants whereas the ICSI corpus has separate audio tracks for each participant. Thus, especially the long periods of silence on each individual channel are new to the model. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=8cm]{imgs/conf_matrix/init_eval_all.png}
    \caption{Confusion matrix for initial evaluation}
    \label{fig:initial-conf-matrix}
\end{figure}

Looking at the confusion matrix in (\autoref{fig:initial-conf-matrix}) we notice that misclassifications are almost evenly split between silence and speech regions. This is surprising because I expected silence to be easier to separate from laughter than speech.
Further evaluation and insights also referring back to this initial evaluation can be found in \autoref{cha:experiments}.
In response to this finding, I decided to retrain the model on the ICSI corpus. 


\chapter{Retraining the model on the ICSI meeting corpus} \label{cha:retraining}
\section{Adapting Gillick et al's training code} 
The first approach was to adapt Gillick et al.'s published code \citep{gillick-codebase}. 


After looking at the code in more detail, I realised that parts of it were cumbersome and disregarded current standards of pytorch programs. 
My second supervisor agreed that he does not consider the code as well written.
For example, they used a manual batch counter for training steps instead of using epochs.  
Design decisions like this made the code harder to read and debug. 

Despite the concerns about Gillick et al.'s code mentioned above, I decided to adapt their existing code for two main reasons:
Firstly, I had never worked on a machine learning project of this scale before. Thus, I considered it safer to use Gillick et al's code, despite the issues mentioned, as it is the backbone of a published paper from Interspeech 2021 \citep{gillick2021robust}. 
Secondly, I thought that using their code and adjusting it slightly would allow for easier comparison between the two original and retrained models. 

From looking at their code I created a diagram of their data-pipeline (\autoref{fig:gillick-data-pipeline}). The raw inputs per conversation are one audio track and two transcription files. 
The following two steps are only done once.
Firstly, all audio tracks are preloaded into memory and stored as a hash-map containing the serialised audio data. This only needs to be done once. Then every time the model is trained the hash-map is loaded into memory to avoid disk accesses which slow down the training. 
Secondly, Gillick et al. filter out all laughter segments and store them in a table as shown in \autoref{fig:gillick-data-pipeline}. Note that tables are also referred to as dataframes in this thesis; this term stems from the commonly used data-analysis library \href{https://pandas.pydata.org/}{pandas}.
Each segment is stored with its start-time, duration, audio path and label where 1 means laughter and 0 means non-laughter. Additionally, a subsample region is stored defined by a start time and duration. 
Gillick et al. add the same amount of non-laughter segments to this table. 

\begin{figure}
    \centering
    \includegraphics[width=14cm]{imgs/diagrams/Gillick_et_al_data_pipeline.png}
    \caption{Gillick et al.'s data pipeline}
    \label{fig:gillick-data-pipeline}
\end{figure}

The hash-map, as well as the table, are the inputs for a pytorch Dataset. This dataset defines the data structure for the pytorch DataLoader which is used during training to sample the batches.

I tried to create a hash-map for ICSI's audio data but ran out of memory. Since the ICSI corpus contains single audio tracks for each participant the total size is larger than the Switchboard dataset. In addition, the sample rate is twice as high.

The following estimates show the difference in storage space needed to preload the whole ICSI/Switchboard corpus as .wav-files. We assume a bit rate of $16bits=2bytes$ per sample.

There are 260 hours of raw audio sampled at 8khz which can be estimated as:

\[ \frac{260hours * 3600 seconds * 8000 samples per second * 2 bytes}{1024^3} \approx 13.95 GB \]

Assuming six participants per meeting on average we have 72*6=432 hours of audio sampled at 16khz:

\[ \frac{432 hours * 3600 seconds * 16000 samples per seconds * 2bytes}{1024^3} \approx 46.35 GB \]

This shows that using a machine with lots of RAM would allow loading the whole audio data into memory. 
Regardless of my missing access to such a machine I wanted to find a solution that requires less RAM. Thus, I decided to rewrite the DataLoader from scratch. 

After adapting the training code for about a week I ran into a major problem. Training the model was slow. The model trained around 30 batches of audio in two hours, each consisting of 32 audio tracks. 
With training samples of one second each, this equates to one sample every eight seconds.
This training performance on a GPU machine is significantly slower than the inference performance on a CPU-only machine (\autoref{tab:rtf}).

After some further investigation, I realised that the data-loading was the bottleneck. I did a crude analysis to check how long it takes to load the data and found that it's unacceptably slow, I could only load 30 batches of 32 one-second segments in two hours. This equates to eight seconds per minute.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
    offset[s] & average time[s] & total time[s] & iterations run \\
    \hline
    0  & 0.15 & 4.40 & 30    \\
    300 & 0.34 & 10.16 & 30  \\ 
    700 & 0.59 & 17.72 & 30  \\
    1000 & 0.78 & 23.36 & 30 \\  
    3000 & 2.03 & 60.79 & 30 \\
    5000 & 3.26 & 97.65 & 30 \\
    \hline
    \end{tabular}
    \caption{Loading a 1s segment from different offset using libros. File loaded: Btr002:chan3. Duration: 5318s}
    \label{tab:librosa-loading-times}
\end{table}

The main reason for slow data loading was missing pre-loading. Thus, each short segment needed to be loaded separately. 
I also created a \href{https://github.com/LasseWolter/laughter-detection-icsi/tree/main/misc_scripts}{naive-librosa-test} that loaded data from different offsets in an audio file. It turned out that the loading time of a sample of an audio track grows proportionally with the offset at which this sample is located (\autoref{tab:librosa-loading-times}). 
This is especially problematic because the average meeting length of 58 minutes \citep{icsi-ldc} is significantly longer than the 6.5 minutes for Switchboard recordings \citep{switchboard-ldc}.


My second supervisor suggested the use of Lhotse \citep{zelasko2021lhotse} to speed up the data-loading. Lhotse is a new python library for speech and audio data preparation. It is still in development but at a state where it's already usable.

\section{Creating a data pipeline using Lhotse}
This section outlines how I used Lhotse to recreate the data pipeline to speed up data-loading. I first give a brief introduction into Lhotse and why it is suitable for our use case. Then I talk about some adjustments I needed to make to the original Lhotse code. After that, I present my final data-pipeline which is part of published \coderepo.

\subsection{Lhotse} \label{sec:lhotse}
Two main features that make Lhotse suitable for my use case are:
\begin{enumerate}
    \item Simplified loading of popular corpora like ICSI
    \item Sample creation without actually loading audio data from disk
\end{enumerate}

Lhotse provides so-called \textit{recipes} which simplify working with popular corpora like the ICSI corpus.
These \textit{recipes} are python scripts that create \textit{manifests}.
\textit{Manifests} are representations of the entire corpus that only contain meta data of the recordings as well as the corresponding supervisions, i.e. transcriptions.
Using those manifests Lhotse can then create samples - called \textit{Cuts} in Lhotse - without loading any audio or transcript data into memory. 

Due to Lhotse being in development I needed to make some adjustments myself. The recipe for the ICSI corpus was not part of the latest release \textit{v0.12} which meant that I had to work with the development version. 
I fixed some bugs in the ICSI recipe to make it work. 
I ran into some other issues with Lhotse and decided to debug some of them. Some of the debugging was not strictly necessary for my project. 
Nevertheless, I decided to spent the time on this because I wanted to contribute to this open source project which has the potential to be useful in the field of audio and speech processing in the future (list contributions?). A list of my contributions, containing links to the corresponding pull requests can be found in \autoref{app:lhotse-contrib}.

\subsection{Data pipeline} \label{sec:ml-data-pipeline}

\autoref{fig:data-pipeline} shows the data pipeline.
Initially, Lhotse's ICSI-recipe takes the raw data and creates a manifest representing the ICSI corpus (\autoref{sec:lhotse}).
Besides this manifest creation there are two main functions in the data pipeline: \verb|create_data_df| and \verb|compute_features|. 
\verb|compute_features| has two parts.
The first part of \verb|compute_features| uses the ICSI-manifest to create a feature representation of all audio tracks in the corpus. Following standards in the ASR community, mel spectrogram features of shape 100x40 are used (\autoref{fig:feature-sample}). The impact of different feature representations is further discussed in \autoref{cha:real-time}. 
Manifest creation and feature computation for the whole corpus, marked in blue, only need to happen once.
Note that if the feature representation is changed, this part of \verb|compute_features| needs to be run again to update the features of the whole corpus.


\begin{figure}[h!]
    \centering
    \includegraphics[width=15cm]{imgs/diagrams/Pipeline.drawio.png}
    \caption{Data Pipeline}
    \label{fig:data-pipeline}
\end{figure}

The remaining parts, marked in orange, need to be run every time the model is trained on a new subset of the corpus.
\verb|create_data_df| creates the three dataframes of the shape shown in \autoref{fig:gillick-data-pipeline}, one for each split.
These dataframes are then input into the second part of the \verb|compute_features| function which take the feature representation of the whole corpus and extract the parts defined in the corresponding dataframe. This is guaranteed to be fast because no actual features need to be computed at this stage.
We end up with three sets of features, one for each split. 
These sets are loaded during training time which means that no data-loading or feature computation needs to happen during training itself.

\begin{figure}[h!]
    \centering
    \includegraphics[width=14cm]{imgs/sample_fbank_feat.png}
    \caption{Example feature representation of a 1s laughter segment}
    \label{fig:feature-sample}
\end{figure}

\href{https://github.com/LasseWolter/laughter-detection-icsi/blob/main/Demo.ipynb}{This jupyter notebook} demonstrates a simplified version of how \verb|compute_features.py| computes features. 
The main difference is that the provided example loads the audio file directly into a Lhotse Cut whereas \verb|compute_features.py| loads already computed feature representation of the whole meeting and only selects the requested subset. In practice, this is a lot faster for computing large amounts of features.

I did not make use of the automatically created supervisions by the ICSI recipe because I already created the \verb|create_data_df| function at an earlier stage and considered reusing it the easier approach.
The generated dataframes contained all information needed for a supervision segment: the channel, the start and end time of a segment as well as its label. 
I adapted a pytorch Dataset\footnote{Dataset here does not refer to actual data. It refers to one of the two main pytorch primitives for data handling: \href{https://pytorch.org/tutorials/beginner/basics/data_tutorial.html}{Datasets}} for \href{https://lhotse.readthedocs.io/en/latest/datasets.html#lhotse.dataset.vad.VadDataset}{voice activity detection} already provided by Lhotse.
For future work using the supervision segments provided by Lhotse can simplify the pipeline even further by making the \verb|create_data_df| function obsolete. 

\section{Training}
Given the data-pipeline outlined in \autoref{sec:ml-data-pipeline} the actual training was straightforward. For each different subset of the corpus I needed to rerun the \verb|create_data_df| and \verb|compute_feature| functions to create the features. 
These features were loaded into the modified VAD Dataset which I called LAD: laugh activity detection.
During training time data is extracted from this dataset using the SimpleCutSampler provided by Lhotse. 
This new approach can process 44000 batches in 2 hours compared to the 30 batches of my initial training approach which allows for high training speed without having to load all the raw audio into memory.
To allow for director comparisons, I used the same ResNet architecture as \citeauthor{gillick2021robust}.
The performance of this model trained on different subsets of the ICSI corpus is investigated in the next section.

\chapter{Investigating the impact of training data on model performance} \label{cha:experiments}
Having representative training data is essential for machine learning to work well. Training on the whole ICSI corpus would take a long time and was not feasible for my project.
Thus, the following experiments investigate how the selection of a corpus-subset impacts the model's performance. 
Due to significant time spent on task already outlined in the thesis, I did not have the time to do an exhaustive grid search. Hence, the results of these experiments should be considered as pointers for future research, not an optimal solution.

The structure of training data was investigated in two dimensions: 
\begin{enumerate}
    \item class balance between laughter and non-laughter samples
    \item diversity of non-laughter samples 
\end{enumerate}

All evaluations in this section are done on the development set of the ICSI corpus as defined in \autoref{sec:preprocessing}.

\section{Experiment 1 - Investigating class balance} \label{sec:exp-1}
In the first set of experiments, different ratios between laughter and non-laughter segments in the training data were evaluated. 
The non-laughter segments were selected on a random basis from the remaining audio tracks after subtracting the laughter and invalid regions (\autoref{table:segment-types}). 
I tried three different ratios between laughter and non-laughter segments: 1-to-1, 1-to-10 and 1-to-20.
All samples had a duration of one second and no subsampling or feature augmentation was applied during training.

\subsection{Method}
To select non-laughter segments across a variety of meetings \verb|create_data_df| uses the following selection process.
For each laughter segment, a non-laughter segment of the same duration is sampled from the same meeting. The sampled channel within that meeting is chosen at random.
If the laughter segment is longer than one second, both laughter and non-laughter segments are truncated to a random one-second region of their segment. 
If the segment is shorter than one second, it is stored unmodified and padded during feature computation.
All these segments are stored in a table as explained in \autoref{sec:ml-data-pipeline}.

\subsection{Results} \label{sec:exp1-res}

\begin{figure}[h!]
    \centering
    \includegraphics[width = 3.8in]{imgs/prec-recall/random/dev_compare_class_balance_dev_set.png}
    \caption{Precision-Recall-Curve for different ratios between laughter and non-laughter}
    \label{fig:prec-recall-rand}
\end{figure}

\autoref{fig:prec-recall-rand} shows the performance of in comparison to the baseline, which is the evaluation on the pre-trained model from \autoref{sec:model-eval}. The data points displayed are precision and recall for 21 evenly space thresholds from 0 to 1. All models trained on the ICSI corpus outperform the baseline. The performance between the three new models is subtle. Depending on the location on the precision-recall curve a different model performs best.

To get a more detailed understanding of the classification \autoref{fig:conf-matrix-rand} shows a confusion matrix for different thresholds. 
Note that a row full of zeros means that there are no laughter predictions for this threshold.
The first observation from the confusion matrices is that most misclassifications are in silence regions, regions that have no transcription. Other than that we note that the outcomes for 1-to-1 and 1-to-10 ratios are quite similar whereas 1-to-20 diverges from the two. Disregarding the thresholds and considering only classification accuracies, the confusions of 1-to-20 are similar to the other two: e.g. threshold 0.3 for 1-to-20 and 0.6 for 1-to-1 and 1-to-10.


\begin{figure}[h!]
\begin{tabular}{cc}
\hspace*{-2cm}                                                           
\subfloat[1-to-1]{\includegraphics[width = 3.5in]{imgs/conf_matrix/random/1_to_1_random.png}} &
\hspace*{-1cm}                                                           
\subfloat[1-to-10]{\includegraphics[width = 3.5in]{imgs/conf_matrix/random/1_to_10_random.png}}\\ 
\hspace*{-2cm}                                                           
\subfloat[1-to-20]{\includegraphics[width = 3.5in]{imgs/conf_matrix/random/1_to_20_random.png}} &
\hspace*{-1cm}                                                           
\subfloat[baseline]{\includegraphics[width = 3.5in]{imgs/conf_matrix/random/baseline.png}}\\ 
\end{tabular}
\caption{Confusion matrices for different ratios between laughter and non-laughter}
\label{fig:conf-matrix-rand}
\end{figure}

\subsection{Analysis} 
The purpose of the experiment was to investigate different class balances. 
The precision-recall curve does not give much insight into this, as all models show a similar performance. Even though we see a difference in the plotted data points, e.g. the model trained on the 1-to-20 training data has more data points in the area of $precision >70\%$.
This difference needs to be seen in context. Thresholds are applied as a post-processing step that does not actually affect the model output itself. 
Having more data points in the area of $precision > 70\%$ does not mean that the model has higher precision. Data points in this region could be achieved with the other models by adjusting their thresholds accordingly.
Other than that we can only conclude that the models trained on the ICSI dataset perform significantly better than the model trained on the Switchboard dataset, which is the expected behaviour.

The confusion matrices give us an interesting insight. Whereas the baseline has an even split between speech and silence, the three newly trained models rarely misclassify speech. 
The non-transcribed regions are the main source of misclassification. This brings up the same question I raised during the initial evaluation. Why is the model not able to confidently separate laughter from silence?

\section{Experiment 2 - Investigating non-laughter diversity}
The second set of experiments investigated the impact of the different types of non-laughter on the model performance. 
Thus, the number of non-laughter segments was fixed to 20. 20 was chosen to maximise the amount of non-laughter segments and be able to use the 1-to-20 results from \auotref{sec:exp-1} as a baseline.
The general selection and feature computation process was similar.
The only difference was that non-laughter segments were not chosen at random but a fixed percentage of non-laughter segments was assigned to each of the three types of non-laughter: speech, noise and silence.
\autoref{tab:exp-2-subclass-split} shows the different percentages tried.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        model & laughter & speech & silence & noise \\
        \hline
        a & 1 & 14 & 2 & 4 \\
        b & 1 &  2 & 14 & 4\\
        c & 2 & 4  & 14 & 2\\ 
        \hline
    \end{tabular}
    \caption{Experiment 2: Splits between laughter and non-laughter subclasses}
    \label{tab:exp-2-subclass-split}
\end{table}

\subsection{Results}
\begin{figure}[h!]
    \centering
    \includegraphics[width = 3.8in]{imgs/prec-recall/structured/dev_compare_class_balance_dev_set.png}
    \caption{Precision-Recall-Curve for different splits of non-laughter subclasses}
    \label{fig:prec-recall-struc}
\end{figure}


\begin{figure}[h!]
\begin{tabular}{cc}
\hspace*{-2cm}                                                           
\subfloat[model-a]{\includegraphics[width = 3.5in]{imgs/conf_matrix/structured/exp-a.png}} &
\hspace*{-1cm}                                                           
\subfloat[model-b]{\includegraphics[width = 3.5in]{imgs/conf_matrix/structured/exp-b.png}}\\ 
\hspace*{-2cm}                                                           
\subfloat[model-c]{\includegraphics[width = 3.5in]{imgs/conf_matrix/structured/exp-c.png}} &
\hspace*{-1cm}                                                           
\subfloat[baseline]{\includegraphics[width = 3.5in]{imgs/conf_matrix/structured/1_to_20_random.png}}\\ 
\end{tabular}
\caption{Confusion matrices for different splits of non-laughter types:  \autoref{tab:exp-2-subclass-split}}
\label{fig:conf-matrix-struc}
\end{figure}

Looking at the precision-recall curve in \autoref{fig:prec-recall-struc}, we see that \texttt{model-a} performs worse than the baseline. \texttt{model-c} performs similar to the baseline with worse performance in the area of $recall > 60\%$. \texttt{model-b} performs slightly better for the area $0.3<recall<0.6$, but worse for recalls higher than this.
We note that all data points for \texttt{model-b} and \texttt{model-c} are squashed in the area of $0.3 < recall < 0.7$ and $0.1 < precision < 0.6$.

\autoref{fig:conf-matrix-struc} compares the confusion matrices. The general structure is similar to the matrices in \autoref{fig:conf-matrix-rand}.
Most of the misclassifications are silence segments. 
The squashing visible in the precision-recall curve becomes evident in the matrices as well. Laughter percentages for the displayed thresholds are low. 
Further, the bright colours in the speech column matrix for \texttt{model-a} show that it retrieves a lot of speech samples. 
Lastly, we can see that \texttt{model-c} has slightly more speech misclassifications than the other three models.


\subsection{Analysis}
The goal of these experiments was to analyse the impact of diversity within the non-laughter segments. The results clearly show that using only a few silence segments decreases model performance significantly. The impact of slightly changing amounts of noise, speech and laughter but keeping the number of silence segments constant yielded a small performance impact. 
Thus, only considering this set of experiments, changing the number of silence segments in the non-laughter class has the most impact. 
As mentioned before this is not a definitive conclusion but merely a result of this small set of experiments. 

The confusion matrices in \autoref{fig:conf-matrix-struc} do not add much to the insights from experiment 1. The higher number of misclassified speech segments in \texttt{model-c} should be noted but cannot be put into context with other observations.


\section{Investigating silence segments}
Both, the initial evaluation (\autoref{cha:model-evaluation}) and the experiments(\autoref{cha:experiments}) raise questions about the silence segments in the corpus. Thus, I decided to manually investigate some audio tracks, to see if I can spot anything unusual. This is a crude analysis to come up with a hypothesis for the large amount of retrieved silence segments.

I randomly selected 5 from the 468 audio tracks and plotted their audio-wave (\autoref{fig:rand-audio-track}). The selected tracks are plotted in this order:
\begin{enumerate}
    \item Bmr027: chan5
    \item Bdb001: chan4
    \item Btr001: chan7
    \item Bns003: chan0
    \item Bmr018: chan1
\end{enumerate}

\begin{figure}[h!]
    \centering
    \includegraphics[width=14cm]{imgs/plotted_audio_waves.png}
    \caption{Audio waves of 5 random audio tracks from the ICSI corpus}
    \label{fig:rand-audio-track}
\end{figure}
Depending on the activity and looking at the corresponding transcript I listened to segments of the recordings that were supposed to be silent. In all cases, except for \verb|Bmr027:chan5|, I was able to clearly hear the other participants speaking. I could understand and follow the conversation by just listening to the recording of the close-distance microphone of a single participant. 
\verb|Bmr027:chan5| (the top recording in \autoref{fig:rand-audio-track}) was recorded with a different headset than the other four tracks which is likely the reason why there is actual silence on this track.
\verb|Bns003:chan0| (highlighted in \autoref{fig:rand-audio-track}) is the most problematic. There is almost no difference between the volume of the participant speaking to the rest of the meeting. For example, there is no transcribed activity for this channel for the first four minutes which is not visible in the audio wave.

The hypothesis I draw from this is that my assumption that everything not transcribed is silence, is wrong. Thus, the areas classified as silence by my preprocessing (\ref{sec:preprocessing}) are not actually silence a lot of the time. They are quieter recordings of speech by other participants. 
Further work should investigate if this hypothesis is true and how it can be dealt with. 



\chapter{Laughter detection in real-time} \label{cha:real-time}
Due to time constraints this chapter only contains theoretical considerations and pointers for future work. 
For detecting laughter in video meetings the model needs to work in real-time and with low computational cost. Real-time here means low latency.
High retrieval accuracy becomes useless if detected laughter is only fed back to the audio stream after a few seconds.
Further, we have to keep in mind that a video meeting application is used on end-user devices with limited computing power. 

\begin{table}[]
    \hspace{-2cm}
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    & \multicolumn{2}{|c|}{i5-6500 CPU @ 3.20GHz} &
    \multicolumn{2}{|c|}{i5-8500 CPU @ 3.00GHz} & 
    \multicolumn{2}{|c|}{GeForce GTX 1060 6GB} \\ 
    \hline
    audio duration & iterations & av-RTF &
    iterations & av-RTF & iterations & av-RTF \\
    \hline
    3s & 20 & 1.31   & 20 & 0.63  & 20 & 0.14  \\
    30s & 20 & 1.41  & 20 & 0.84  & 20 & 0.10 \\
    120s & 10 & 1.49 &  10 & 0.81  & 20 & 0.10 \\
    300s &&&&                     & 10 & 0.10 \\
    \hline
    \end{tabular}
    \caption{Average real-time factor for inference on different machines (2x CPU, 1xGPU)}
    \label{tab:rtf}
\end{table}


There are five factors that influence the real-time nature of the model:
\begin{enumerate}
    \item programming language
    \item computational power of the end user's device
    \item model's complexity
    \item model's window-size
    \item model's minimum length parameter 
\end{enumerate}

The programming language will become important when such a model is integrated with a video call system. Since the work in this thesis does not cover this part of the project, the discussion about the programming language is left for future work.


The limited computational power of the end user's device cannot be directly influenced. It is mentioned in this context because it impacts latency. This impact can be minimised by reducing the model's complexity.
The model presented by Gillick et al. which is used in this thesis, a ResNet architecture with 18 layers, is not computationally efficient. There is research investigating the reduction of the computational footprint with minimal impact on performance \cite{sorensen2020depthwise}. Future work should review such literature and see how it can be applied to this project. 

The feature representation is another important design decision that affects the latency.
The experiments in \autoref{cha:experiments} all use mel spectrogram-features of shape 100x40, which means 100 samples and 40 filters. 
Since our model uses 100samples per feature we have a window-size of 100, which equals 1s.
Assuming splitting this window in the middle, similar to \autoref{fig:knox_window}, this meant around 0.5s before and 0.5s after the considered frame. This is an unacceptable latency.
There are two possible solutions: reducing the window-size, which means losing context information, or using an uneven split, e.g. three-fourths before the frame and one fourth after. 
Both of these options should be investigated by future work.

The last factor mentioned is the minimum length. This is a parameter used by Gillick et al's model. It prevents the retrieval of a few milliseconds which are classified as laughter. For a real-time model this will be done at the cost of latency. If a minimum length is specified the model needs to wait at least that amount of time before a laughter occurrence is approved. Thus, the minimum latency is the duration of the minimum length itself.
Future work should investigate if a minimum length parameter is needed at all or if a smarter use of the context window can avoid this additional parameter.

Thus, assuming a minimum length $m$ and a window-size $w$ a lower bound of a model's latency is given by: $m + \frac{1}{2}w$.

To conclude this section, I provide a baseline (\auotref{tab:rtf}) for future work that estimates the latency of Gillick et al's model with a window size of 1s. 
The latency is estimated by computing the real-time-factor, which is calculated as: 
$$ RTF = \frac{time\ to\ process\ a\ segment}{duration\ of\ the\ segment} $$

It should be noted that the minimum length parameter mentioned before does not impact the RTF. 
Since the minimum length is a post-filter applied on the model output, for longer segments the overhead presented for a real-time model does not apply. 


\chapter{Practicality of laughter recognition in video meetings}
In this section I will talk about the practicality of a laughter recognition feature integrated in a video-call system. 
This section is split into two parts: 
\begin{enumerate}
    \item Practicality issues of such a system
    \item Alternative solutions to using a machine learning approach 
\end{enumerate}

\section{Practicality issues} \label{sec:general-pract}
Even though users will be able to opt-out of using this feature, it is important to think about privacy. If most users do not feel comfortable using such a system, it will significantly affect its practicality.  
The audio of every meeting participant will be constantly analysed. Even if the system promised to discard all audio immediately, users might feel uncomfortable using such a system. False positives in particular are a problem. If the system detects something as laughter and feeds it into the audio stream it is irreversible. If the false positive is a sensitive conversation with a friend, the cost of this issue is huge. 
The extent of this issue can be limited by tuning the model threshold in such a way that the False positive rate is minimised. Nevertheless, the use case outlined above will always remain possible. 

Another general problem is the missing detection of laughter sentiment. There is laughter in response to a joke which will be pleasing to the speaker. In contrast, laughter that makes fun of the speaker might seriously affect the speaker's confidence. I talked to a member of staff from the university who told me that she would be afraid of using such a system for this reason. 
For a machine learning model to pick up the sentiment conveyed by a laughter occurrence is a completely different task. Even if this succeeded to a certain degree, false positives would be an issue again. 


\section{Alternatives}
Considering the privacy concerns mentioned in \autoref{sec:general-pract}, we can think about alternative sensory data. 
Constantly analysing video input will likely yield similar discomfort for participants. Further, video inputs are often turned off during larger video conferences.
\citeauthor{cosentino2016quantitative}  also mention the privacy implications of video and audio inputs.
Thus, they suggest the use of individual wearable sensors to address these concerns. For our project this approach wasn't feasible due to the lack of available sensory data.
It is also impractical to deploy such a system on a large scale. Participants join video conferences from everywhere around the world, where wearable sensors are not available to them. Lastly, it is debatable if a wearable sensor brings more comfort to participants than being listened to all the time. 

During a presentation another student suggested an interesting idea. He suggested constantly analysing all audio inputs of the meeting without looking for laughter specifically. Considering that usually only one person is speaking, audio activity on lots of inputs synchronously suggests some common activity, e.g. laughter or applause. Trying to find identify patterns like this is an interesting alternative worth thinking about. 

\chapter{Future Work}
The majority of my work has been the creation of a data and evaluation pipeline to work with laughter in the ICSI corpus \citep{morgan2001meeting}. 
Thus, continued experimentation with different machine learning models, different parameters and class balances can build upon my insights and make use of my \coderepo.
This extends beyond the purpose of the project of a laughter recognition system for video meetings but can be useful for further research in the broader area of laughter detection in general. 
As mentioned in \autoref{cha:bg}, lots of papers didn't publish their code.
Thus, especially people with little experience in this field can benefit from my existing code. 
For such broader usage of my code some refactoring and the creation of proper documentation would be necessary.
Some parts of the code, especially \verb|train.py|, still contain large parts of Gillick et al.'s code that I criticised as not adhering to common pytorch standards. 


% \chapter{Lessons Learned}

% \section{Use of published code}
% In hindsight, I would be more careful in choosing the codebase I base my work on.
% Working with sparsely documented code that doesn't adhere to certain standards (e.g. for pytorch training) slowed down my working progress. 
% Missing experience in the field and the paper's publication at Interspeech 2021 made me think that it was a good codebase to work with.
% Only during retraining the model and working more closely with the codebase I realised certain bugs and issues with the original code, including:
% \begin{enumerate}
%     \item unnecessary long training code (\autoref{sec:retraining})
%     \item lots of unused function
%     \item a bug that the model outputs probabilities out of bounds ($<0$ or $>1$) (\autoref{sec:experiments})
%     \item sub-optimal code like recreation of a large object inside a for-loop even though it's independent of it
%     \item mistakes in use of python operators like slicing - e.g. using \texttt{min} \texttt{np.min(probs[i:i+1])} even though this only returns 1 probability - end index is non-inclusive
% \end{enumerate}

% The bug with probabilities out of bounds was cause by a lowpass filter implemented in \verb|laugh_segmenter.py|. I did not investigate this further because neither the code nor the paper states why this filter is needed. Thus, I removed it.

% \chapter{Other}

% The computations for ML training and evaluation were run on a slurm cluster of the university \citep{yoo2003slurm, tange2011gnu}.


\bibliographystyle{plainnat}
\bibliography{mybibfile}

%% You can include appendices like this:
\appendix
\chapter{First appendix}

\section{Lhotse contributions} \label{app:lhotse-contrib}
Pull Requests in chronological order 
\begin{enumerate}
    \item minor documentation fix in ICSI recipe: \href{https://github.com/lhotse-speech/lhotse/pull/544}{PR 1}
    \item added missing microphone channels to ICSI recipe: \href{https://github.com/lhotse-speech/lhotse/pull/555}{PR 2}
    \item added warning for certain case of feature computation: \href{https://github.com/lhotse-speech/lhotse/pull/561}{PR 3}
    \item improved ICSI recipe download structure: \href{https://github.com/lhotse-speech/lhotse/pull/583}{PR 4} and \href{https://github.com/lhotse-speech/lhotse/pull/592}{PR 5}
\end{enumerate}
%
% Markers do not have to consider appendices. Make sure that your contributions
% are made clear in the main body of the dissertation (within the page limit).

\end{document}
